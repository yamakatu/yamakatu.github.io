<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: es-kibana | Yamakatu as a Service]]></title>
  <link href="http://yamakatu.github.io/blog/categories/es-kibana/atom.xml" rel="self"/>
  <link href="http://yamakatu.github.io/"/>
  <updated>2014-12-02T02:07:04+09:00</updated>
  <id>http://yamakatu.github.io/</id>
  <author>
    <name><![CDATA[yamakatu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ElasticsearchでCSVからインデクシングする]]></title>
    <link href="http://yamakatu.github.io/blog/2014/04/30/escsvriver/"/>
    <updated>2014-04-30T16:28:37+09:00</updated>
    <id>http://yamakatu.github.io/blog/2014/04/30/escsvriver</id>
    <content type="html"><![CDATA[<!-- more -->


<br/>


<p>インデクシングしたいデータがCSVだった場合、CSVからjsonへの変換は意外と面倒だったりしませんか。</p>

<p>そんな場合、CSV River Pluginを使ってCSVから直接インデクシングがオススメです。</p>

<p><a href="https://github.com/AgileWorksOrg/elasticsearch-river-csv">CSV River Plugin</a></p>

<h3>インストール</h3>

<p>(ver 2.0.1の場合。ESとのバージョン対応は上記Githubで要確認)</p>

<p><code>bash
bin/plugin -install river-csv -url https://github.com/AgileWorksOrg/elasticsearch-river-csv/releases/download/2.0.1/elasticsearch-river-csv-2.0.1.zip
</code></p>

<h3>登録</h3>

<p>こんな感じ。他のオプションは上記ページで要確認</p>

<p>```bash
curl -XPUT localhost:9200/river/my_csv_river/meta -d ’ {
  &ldquo;type&rdquo; : &ldquo;csv&rdquo;,
  &ldquo;csv_file&rdquo; : {</p>

<pre><code>"first_line_is_header":"true",
"folder" : "/tmp",
"filename_pattern" : ".*\\.csv$"
</code></pre>

<p>  },
  &ldquo;index&rdquo; : {</p>

<pre><code>"index" : "index_name",
"type" : "type+name",
"bulk_size" : 1000,
"bulk_threshold" : 10
</code></pre>

<p>  }
}’
```</p>

<h3>罠</h3>

<p>しかし罠がありました。</p>

<br/>


<p>GithubのREADME.mdには一言も書いてませんが、処理されたCSVファイルは以下の命名規則でRenameされます。</p>

<br/>


<p>（hoge.csvの場合）</p>

<ul>
<li>読み込み中：hoge.csv.processing</li>
<li>読み込み済：hoge.csv.processing.imported</li>
</ul>


<br/>


<p>登録のjsonを見れば雰囲気でわかりますが、folder以下のfilename_patternで指定したファイルが対象になります。</p>

<p>そのため、</p>

<p><code>sh
“filename_pattern” : “.*”
</code></p>

<p>みたいな正規表現で設定すると、読み込み済みのCSVファイルが、次回polling時に読み込み対象になります。</p>

<p>pollingはデフォルト60minなので（変更可）、1時間ごとにすべてのファイルが再インデクシングされます。</p>

<p>デフォルト値が上記正規表現なので、通常であればこのままにしとくのが吉です。</p>

<br/>


<p>変更する場合、登録する正規表現は</p>

<p>.csv$</p>

<p>で終わるようにしましょう。</p>

<br/>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[elasticsearch-hadoopをもうちょい調べて遅い理由が少しわかった]]></title>
    <link href="http://yamakatu.github.io/blog/2014/04/25/eshadoop/"/>
    <updated>2014-04-25T17:20:45+09:00</updated>
    <id>http://yamakatu.github.io/blog/2014/04/25/eshadoop</id>
    <content type="html"><![CDATA[<!-- more -->


<h2>■これまでの話</h2>

<p>第4回Elasticsearch勉強会でElasticsearch-hadoopについて発表させて頂きましたが、その際、評価としてHiveの実行速度をHDFSをストレージとして利用した場合と比較してお見せしました。</p>

<br/>


<p>ここらへんです。</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/33755795?startSlide=18" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/yamakatu/elasticsearchhadoop" title="elasticsearch-hadoopをつかってごにょごにょしてみる" target="_blank">elasticsearch-hadoopをつかってごにょごにょしてみる</a> </strong> from <strong><a href="http://www.slideshare.net/yamakatu" target="_blank">Katsushi Yamashita</a></strong> </div></p>

<br/>


<p>で、ちょっといくらなんでも遅すぎるな、と思って調べていたらわかったことがあったのでメモ。</p>

<h2>■わかったこと</h2>

<p>結論から言うと、elasticsearch-hadoopはRead時はPrimary Shardの数しかMapタスクを生成しない。</p>

<br/>


<p><img src="/images/majide.jpg" title="&lsquo;&rsquo; &lsquo;&rsquo;" ></p>

<br/>


<p>これはelasticsearch-hadoopのここら辺のソースを読んでもわかります。</p>

<p>・EsInputFormat.java getSplitsメソッド</p>

<p>```java</p>

<pre><code>    ShardInputSplit[] splits = new ShardInputSplit[targetShards.size()];

    int index = 0;
    for (Entry&lt;Shard, Node&gt; entry : targetShards.entrySet()) {
        Shard shard = entry.getKey();
        Node node = entry.getValue();
        splits[index++] =
                    new ShardInputSplit(node.getIpAddress(), node.getHttpPort(), node.getId(), node.getName(), shard.getName(), savedMapping, savedSettings);
    }

    log.info(String.format("Created [%d] shard-splits", splits.length));
    return splits;
</code></pre>

<p>```</p>

<p>Hadoopは利用するInputFormatのgetSplitsメソッドの返り値の長さで、Mapタスク数を決定します。</p>

<p>このソースを見るとわかるように、Shardのsizeで配列を生成してますね。。</p>

<p>#Hive利用時に実際に利用するInputFormatはEsHiveInputFormatなのですが、EsHiveInputFormatのgetSplitsが内部で上記を呼び出しています。</p>

<br/>


<p>しかし、そもそもちゃんと公式ホームページ見てみると、<a href="http://www.elasticsearch.org/overview/hadoop/">elasticsearch-hadoopのトップページ</a>に書いてありましたw</p>

<h2>■じゃあ、どうすればいいのか</h2>

<p>とりあえずは、インデックス生成の際にShardの数を多くすれば解決します。</p>

<p>しかし、ElasticsearchはShardの数をインデックス生成後は変更できない（はず）なので、微妙な感が否めませんが。。。</p>

<p>もっといい方法はないだろうか。。。</p>

<p>次回のElasticearch勉強会でMapRの<a href="https://twitter.com/nagix">@nagix</a>さんから聞けたりするといいな。</p>

<h2>■追試</h2>

<p>前回の評価では、シャード数が6なので、6つのMapタスクしか生成されてないことになります。
実際、ログを確認したところ、生成されたMapタスク数は6でした。</p>

<p>一方、HDFSを利用した場合はMapタスクは12個生成されていました。</p>

<br/>


<p>TaskTrackerは3台ともmapred.tasktracker.map.tasks.maximumを4に設定しているので、最大で12Mapを並列実行可能です。</p>

<p>また、サーバは4コアマシンですので、CPU的にも3台合わせて12Mapを並列実行できる計算になります。</p>

<p>#実際にはElasticseachとかのプロセスがCPUリソースを消費しますが</p>

<br/>


<p>よって、HDFS利用時は12コアを使用し、elasticsearch-hadoop利用時は6コアしか使用されていないことになります。</p>

<p>Oh,,,ということで再度評価を行いました。</p>

<p>TaskTrackerとElasticsearchが同居したサーバを3台用意し(上記スライドの3番目の評価環境と同じ）、シャード数を12、レプリカ数を2でインデックスを生成して実行しました。</p>

<br/>


<p>結果は520ms ➡ 280ms前後にまで短縮できました。</p>

<p>HDFS利用時の速度とはまだまだ差はありますが、少しはマシになりました。。</p>

<br/>


<p>#そもそも1分でも十分遅いので、1分も5分もそんなに大して変わらないとかなんとか</p>

<h2>■お詫びと訂正</h2>

<p>スライドの10ページ目で「ドキュメント1件で1Mapタスク」とありますが、正しくはMapReduceでreadする場合もHive同様、生成されるMapタスクの数はPrimary Shardの数でした。</p>

<p>お詫びして訂正します。</p>

<p>#一番最初にドキュメント数を極小で試したのですが、その時のドキュメント数とシャード数が同じだったようです。。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[第4回Elasticsearch勉強会で発表してきました]]></title>
    <link href="http://yamakatu.github.io/blog/2014/04/24/es04/"/>
    <updated>2014-04-24T02:00:10+09:00</updated>
    <id>http://yamakatu.github.io/blog/2014/04/24/es04</id>
    <content type="html"><![CDATA[<!-- more -->


<br/>


<p><a href="http://elasticsearch.doorkeeper.jp/events/8865">第4回Elasticsearch勉強会</a>で発表してきました。</p>

<br/>


<p>Kibanaなんかと比較し、これまであまり注目されることのなかったelasticsearch-hadoopが不憫すぎたので、elasticsearch-hadoopについて話してきました。</p>

<br/>


<p>当日の夕方ぐらいから下痢が始まって、自分の前の <a href="https://twitter.com/johtani">@johtani</a> さんの発表中にもトイレに駆け込んだんですが、なんとか発表中は中身を出さずにすみました。</p>

<p>100人を超える参加者だったので、緊張するかなーとおもっていましたが、下痢でそれどこではなかったので、力が抜けていいかんじに発表できました。</p>

<p>ちなみにこれを下痢駆動発表と呼ぶらしいです。</p>

<blockquote class="twitter-tweet" lang="ja"><p>下痢かつさんの下痢駆動発表</p>&mdash; ノートハエ・ノーライフ (@tohae) <a href="https://twitter.com/tohae/statuses/458516157683601408">2014, 4月 22</a></blockquote>


<script async src="http://yamakatu.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script>


<br/>


<p>みなさんも一度、試してみてはいかがでしょうか。</p>

<br/>


<p>・<a href="http://dev.classmethod.jp/server-side/4th-elasticsearchjp/">クラスメソッドさんまとめ</a></p>

<p>・<a href="http://blog.johtani.info/blog/2014/04/21/hold-on-4th-elasticsearch-jp/">大谷さんまとめ</a></p>

<br/>




<iframe src="http://www.slideshare.net/slideshow/embed_code/33755795" width="597" height="486" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/yamakatu/elasticsearchhadoop" title="elasticsearch-hadoopをつかってごにょごにょしてみる" target="_blank">elasticsearch-hadoopをつかってごにょごにょしてみる</a> </strong> from <strong><a href="http://www.slideshare.net/yamakatu" target="_blank">Katsushi Yamashita</a></strong> </div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【Kibana】Histgramで欠損値に0が自動補完されるのがアレだったので、OFFにする機能を実装してプルリクしてマージしてもらったお話]]></title>
    <link href="http://yamakatu.github.io/blog/2014/02/09/kibana-pull-req/"/>
    <updated>2014-02-09T15:40:59+09:00</updated>
    <id>http://yamakatu.github.io/blog/2014/02/09/kibana-pull-req</id>
    <content type="html"><![CDATA[<!-- more -->


<h1>■前提</h1>

<p>Kibana &lt;= kibana-3.0.0 milestone4</p>

<br/>


<h1>■何が起きたか</h1>

<p>去年の12月ぐらいに、「Elasticsearch + Kibana って、ログの可視化に超便利ジャーン！」という感じでKibanaを利用していたところ、Histgramの描画でちょっと微妙な現象に遭遇。</p>

<br/>


<p>例えば、</p>

<br/>


<table border=1><tr align="center"><td width="200">Time</td><td width="100">value</td></tr><tr align="center"><td>2014/11/08T09:00:00</td><td>10</td></tr><tr align="center"><td>2014/11/08T09:30:00</td><td>30</td></tr><tr align="center"><td>2014/11/08T10:00:00</td><td>20</td></tr></table>


<br/>


<p>な感じのデータでHistgramを描画した時、自分はこう描画して欲しかった。</p>

<br/>


<p><img src="/images/kibana-ok.png" title="&lsquo;&rsquo; &lsquo;&rsquo;" ></p>

<br/>


<p>が、描画されるのは以下のようなグラフ。</p>

<br/>


<p><img src="/images/kibana-ng.png" title="&lsquo;&rsquo; &lsquo;&rsquo;" ></p>

<br/>


<h1>■どういうことなのか</h1>

<p>この二つのグラフはご覧の通り、Intervalの値が異なる。前者のグラフは30分、後者のグラフは10分。</p>

<p>んで、KibanaはIntervalごとに値を表示しようとする。</p>

<p>しかし、10分ごとの場合、9:10、9:20などには値が欠損している。</p>

<h3>この時Kibanaは勝手に0を補完する。</h3>

<br/>


<p>いやー気が利くねー（白目</p>

<br/>


<h1>■どう思った？</h1>

<p>確かに0を補完して描画して欲しい場合もあるだろうから、間違ってはいない。</p>

<p>しかし、今回の自分のように、余計なことすんじゃねーよ、という場合も普通にある。</p>

<p>なので、選べるようにした方がよくねー？</p>

<br/>


<p>あと前者のグラフのように、データが一定間隔でかつ欠損がなく、Intervalの値をその間隔に設定できる場合、この事象は発生しない。</p>

<p>だけど、こういう場合であろうとなかろうと、発生しないでほしい。。。</p>

<br/>


<h1>■それでどうしたのか</h1>

<p>Githubでソース読んでたら、実は既にグラフ描画の仕方が選択できるように実装されてた。<a href="https://github.com/elasticsearch/kibana/blob/master/src/app/panels/histogram/timeSeries.js#L103">ここらへん</a></p>

<p>(ちなみにこの描画方式はコード上ではstrategyと呼ばれているので、以下strategy）</p>

<p>なんだけど、今回のような「値のない時間に0を補完しない」というstrategyは当時はなかった。</p>

<br/>


<p>というわけで、実装して、issue作って、プルリク送っときました。<a href="https://github.com/elasticsearch/kibana/pull/742">#742</a></p>

<p>そしたら忘れた頃にマージしてもらえました。</p>

<p><a href="https://github.com/elasticsearch/kibana/blob/master/src/app/panels/histogram/timeSeries.js#L103">上記コード</a>にある_getNoZeroFlotPairsがソレです。</p>

<p>この実装ではIntervalがどんな値だろうが、欠損値に0を補完しなくなります。</p>

<br/>


<p>次のリリースでは乗っかると思われます。</p>

<p>ただ、以前コードを見た時は、そもそもstrategyを選択するUIがなかったので、画面からはまだ使えないかもしれん。。。</p>

<br/>


<h1>■すぐ使いたければどうすればいいのか</h1>

<p>リリースまで待てない人は上記ソースを参考にして改修すると良いと思います。</p>

<br/>


<p>もしくは、とりあえず今すぐちょっと試したい方なんかは、乱暴だけど、</p>

<p>app/panels/histogram/timeSeries.js</p>

<p>の</p>

<p>ts.ZeroFilled.prototype._getMinFlotPairs</p>

<p>を以下のように改修するのが一番手っ取り早いです。</p>

<p>```js
  ts.ZeroFilled.prototype._getMinFlotPairs = function (result, time, i, times) {</p>

<pre><code>if(this._data[time]){
  result.push([ time, this._data[time]]);
}
return result;
</code></pre>

<p>  };
```</p>

<p>#人によっては改修後にブラウザのキャッシュクリアが必要かもしれん</p>

<p>ちなみにコードをいじる場合は、公式サイトからDLしたkibanaだと改修が大変なので、<a href="https://github.com/elasticsearch/kibana/releases">Github内のリリース</a>から落としたコードを利用すると幸せになれると思います。</p>

<br/>


<h1>■まとめ</h1>

<ul>
<li>今のKibanaのHistgramは欠損値に勝手に0を入れる</li>
<li>それをOFFにするオプションを実装して、プルリク送って、マージされた</li>
<li>自分でがんばって書き換えればすぐに使えるよ</li>
</ul>

]]></content>
  </entry>
  
</feed>
